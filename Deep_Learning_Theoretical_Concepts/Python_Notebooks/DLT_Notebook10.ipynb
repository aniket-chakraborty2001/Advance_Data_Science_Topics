{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [],
      "source": [
        "## Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from keras.datasets import mnist\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rnBedU3H8uK"
      },
      "outputs": [],
      "source": [
        "## Mount Google drive folder if running in Colab\n",
        "if('google.colab' in sys.modules):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount = True)\n",
        "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2024MAHE'\n",
        "    DATA_DIR = DIR + '/Data/'\n",
        "else:\n",
        "    DATA_DIR = 'Data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q1e2N5S8MlCU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "85686578-8cdf-4e75-d151-5d153906339a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "---\n",
        "\n",
        "Load MNIST Data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E5kaKFKSIQgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c580d3-a127-4caf-ab44-cc0656f66eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "MNIST set\n",
            "---------------------\n",
            "Number of training samples = 60000\n",
            "Number of features = 784\n",
            "Number of output labels = 10\n"
          ]
        }
      ],
      "source": [
        "## Load MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.transpose(1, 2, 0)\n",
        "X_test = X_test.transpose(1, 2, 0)\n",
        "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "num_labels = len(np.unique(y_train))\n",
        "num_features = X_train.shape[0]\n",
        "num_samples = X_train.shape[1]\n",
        "\n",
        "# One-hot encode class labels\n",
        "Y_train = tf.keras.utils.to_categorical(y_train).T\n",
        "Y_test = tf.keras.utils.to_categorical(y_test).T\n",
        "\n",
        "\n",
        "# Normalize the samples (images)\n",
        "xmax = np.amax(X_train)\n",
        "xmin = np.amin(X_train)\n",
        "X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n",
        "X_test = (X_test - xmin)/(xmax - xmin)\n",
        "\n",
        "print('MNIST set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))\n",
        "print('Number of output labels = %d'%(num_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrXipxwrJ0_8"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdLfiQSlOSUU"
      },
      "source": [
        "---\n",
        "\n",
        "The softmax classifier steps for a batch of comprising $b$ samples represented as the $725\\times b$-matrix (784 pixel values plus the bias feature absorbed as its last row) $$\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}^{(0)},\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(b-1)}\\end{bmatrix}$$ with one-hot encoded true labels represented as the $10\\times b$-matrix (10 possible categories) $$\\mathbf{Y}=\\begin{bmatrix}\\mathbf{y}^{(0)}&\\ldots&\\mathbf{y}^{(b-1)}\\end{bmatrix}$$ using a randomly initialized $10\\times725$-weights matrix $\\mathbf{W}$:\n",
        "\n",
        "1. Calculate $10\\times b$-raw scores matrix : $$\\begin{align*}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\end{bmatrix} &= \\mathbf{W}\\begin{bmatrix}\\mathbf{x}^{(0)}&\\ldots&\\mathbf{x}^{(b-1)}\\end{bmatrix}\\\\&=\\begin{bmatrix}\\mathbf{W}\\mathbf{x}^{(0)}&\\ldots&\\mathbf{W}\\mathbf{x}^{(b-1)}\\end{bmatrix}\\\\\\Rightarrow \\mathbf{Z} &= \\mathbf{WX}.\\end{align*}$$\n",
        "2. Calculate $10\\times b$-softmax predicted probabilities matrix: $$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\text{softmax}\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\text{softmax}\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\text{softmax}(\\mathbf{Z}).\\end{align*}$$\n",
        "3. Predicted probability matrix gets a new name: $\\hat{\\mathbf{Y}} = \\mathbf{A}.$\n",
        "4. The crossentropy (CCE) loss for the $i$th sample is $$L_i = \\sum_{k=0}^9-y^{(i)}_k\\log\\left(\\hat{y}^{(i)}_k\\right) = -{\\mathbf{y}^{(i)}}^\\mathrm{T}\\log\\left(\\mathbf{y}^{(i)}\\right)$$ which leads to the average crossentropy (CCE) batch loss for the batch as:\n",
        "$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+\\cdots+L_{b-1}\\right]\\\\&=\\frac{1}{b}\\left[-{\\mathbf{y}^{(0)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(0)}\\right)+\\cdots+-{\\mathbf{y}^{(b-1)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\right].\\end{align*}.$$\n",
        "5. The computational graph for the samples in the batch are presented below:\n",
        "\n",
        "$\\hspace{1.5in}\\begin{align*}L_0\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(0)} &= \\mathbf{a}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\\qquad\\cdots\\qquad$$\\begin{align*} L_{b-1}\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(b-1)} &= \\mathbf{a}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$\n",
        "6. Calculate the gradient of the average batch loss w.r.t. weights as: $$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left(\\hat{\\mathbf{y}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\text{sample}\\,0}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\text{sample}\\,b-1}\\right)\\\\&=\\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\text{sample}\\,0}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\text{sample}\\,b-1}\\right).\\end{align*}$$\n",
        "which can be written as $\\nabla_\\mathbf{W}(L)=$\n",
        "\n",
        "![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103292&authkey=%21AMoosVj6GqUSvpc&width=660)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9YGwzbz72CZ"
      },
      "source": [
        "---\n",
        "\n",
        "CCE loss and its gradient for the batch samples:\n",
        "\n",
        "$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+\\cdots+L_{b-1}\\right]\\\\&=\\frac{1}{b}\\left[-{\\mathbf{y}^{(0)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(0)}\\right)+\\cdots+-{\\mathbf{y}^{(b-1)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\right].\\end{align*}$$\n",
        "\n",
        "$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)&\\ldots&\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\end{bmatrix}=\\begin{bmatrix}-y_0^{(0)}/\\hat{y}_0^{(0)}&\\cdots&-y_0^{(b-1)}/\\hat{y}_0^{(b-1)}\\\\-y_1^{(0)}/\\hat{y}_1^{(0)}&\\ldots&-y_1^{(b-1)}/\\hat{y}_1^{(b-1)}\\\\-y_2^{(0)}/\\hat{y}_2^{(0)}&\\cdots&-y_2^{(b-1)}/\\hat{y}_2^{(b-1)}\\\\\\vdots\\\\-y_9^{(0)}/\\hat{y}_9^{(0)}&\\cdots&-y_9^{(b-1)}/\\hat{y}_9^{(b-1)}\\end{bmatrix}.\\end{align*}$$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def cce(Y, Yhat):\n",
        "  return(np.mean(np.sum(-Y*np.log(Yhat), axis = 0)))\n",
        "  #TensorFlow in-built function for categorical crossentropy loss\n",
        "  #cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "  #return(cce(Y, Yhat).numpy())\n",
        "\n",
        "def cce_gradient(Y, Yhat):\n",
        "  return(-Y/Yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generic activation layer class:\n",
        "\n",
        "**Forward**:\n",
        "$$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\sigma\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\sigma\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\sigma(\\mathbf{Z}).\\end{align*}$$\n",
        "\n",
        "**Backward**:\n",
        "$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}(L_0)&\\ldots&\\nabla_{\\mathbf{z}^{(b-1)}}(L_{b-1})\\end{bmatrix} &= \\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\cdots&\\nabla_{\\mathbf{z}^{(b-1)}}\\left({\\mathbf{a}}^{(b-1)}\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}\\\\&=\\begin{bmatrix}\\text{diag}\\left(\\nabla_{\\mathbf{z}^{(0)}}\\left(\\sigma\\left(\\mathbf{z}^{(0)}\\right)\\right)\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\ldots&\\text{diag}\\left(\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\sigma\\left(\\mathbf{z}^{(b-1)}\\right)\\right)\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}.\\end{align*}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "ba6WoPMRwGXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_gradient):\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = self.activation(self.input)\n",
        "        return(self.output)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate = None):\n",
        "        return(self.activation_gradient(self.input) * output_gradient[:-1, :])"
      ],
      "metadata": {
        "id": "C21FcWIEwGCN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specific activation layer classes:"
      ],
      "metadata": {
        "id": "JheGWSoKxYWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(z):\n",
        "            return 1 / (1+np.exp(-z))\n",
        "\n",
        "        def sigmoid_gradient(z):\n",
        "            a = sigmoid(z)\n",
        "            return a * (1-a)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_gradient)\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(z):\n",
        "            return np.tanh(z)\n",
        "\n",
        "        def tanh_gradient(z):\n",
        "            return\n",
        "\n",
        "        super().__init__(tanh, tanh_gradient)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(z):\n",
        "            return z * (z > 0)\n",
        "\n",
        "        def relu_gradient(z):\n",
        "            return 1. * (z > 0)\n",
        "\n",
        "        super().__init__(relu, relu_gradient)"
      ],
      "metadata": {
        "id": "PQ5ybz_Yxbef"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smgXLg9p65HV"
      },
      "source": [
        "---\n",
        "\n",
        "Softmax activation layer class:\n",
        "\n",
        "**Forward**:\n",
        "$$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\text{softmax}\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\text{softmax}\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\text{softmax}(\\mathbf{Z}).\\end{align*}$$\n",
        "\n",
        "**Backward**:\n",
        "$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}(L_0)&\\ldots&\\nabla_{\\mathbf{z}^{(b-1)}}(L_{b-1})\\end{bmatrix} &= \\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\cdots&\\nabla_{\\mathbf{z}^{(b-1)}}\\left({\\mathbf{a}}^{(b-1)}\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}\\end{align*}$$\n",
        "\n",
        "![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103299&authkey=%21AIPPR63BJ3UybA8&width=928&height=99)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.array([[10., 20., 30.], [-10., 50., 45.]])\n",
        "print(z)\n",
        "tf.nn.softmax(z, axis = 0).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDpmikNF1jmO",
        "outputId": "9e4de830-5256-409c-ac27-8bf330860c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 10.  20.  30.]\n",
            " [-10.  50.  45.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00e+00, 9.36e-14, 3.06e-07],\n",
              "       [2.06e-09, 1.00e+00, 1.00e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4x1Xn3AbJlNy"
      },
      "outputs": [],
      "source": [
        "## Softmax activation layer class\n",
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    self.output = tf.nn.softmax(input, axis = 0).numpy()\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate = None):\n",
        "    ## Following is the inefficient way of calculating the backward gradient\n",
        "    softmax_gradient = np.empty((self.output.shape[0], output_gradient.shape[1]), dtype = np.float64)\n",
        "    for b in range(softmax_gradient.shape[1]):\n",
        "      softmax_gradient[:, b] = np.dot((np.identity(self.output.shape[0])-np.atleast_2d(self.output[:, b])) * np.atleast_2d(self.output[:, b]).T, output_gradient[:, b])\n",
        "    return(softmax_gradient)\n",
        "    ## Following is the efficient way of calculating the backward gradient\n",
        "    #T = np.transpose(np.identity(self.output.shape[0]) - np.atleast_2d(self.output).T[:, np.newaxis, :], (2, 1, 0)) * np.atleast_2d(self.output)\n",
        "    #return(np.einsum('jik, ik -> jk', T, output_gradient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPFfd1U68dj"
      },
      "source": [
        "---\n",
        "\n",
        "Dense layer class:\n",
        "\n",
        "**Forward**:\n",
        "$$$$\\begin{align*}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix} &= \\mathbf{W}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix}\\\\&=\\begin{bmatrix}\\mathbf{W}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{W}\\mathbf{z}^{(b-1)}\\end{bmatrix}\\\\\\Rightarrow \\mathbf{Z} &= \\mathbf{WX}.\\end{align*}$$$$\n",
        "\n",
        "**Backward**:\n",
        "$$\\begin{align*}\\nabla_\\mathbf{W}(L)&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{W}}(\\mathbf{z}^{(0)})\\times\\nabla_{\\mathbf{z^{(0)}}}(L) +\\cdots+ \\nabla_{\\mathbf{W}}(\\mathbf{z}^{(b-1)})\\times\\nabla_{\\mathbf{z^{(b-1)}}}(L)\\right]\\\\&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{z^{(0)}}}(L){\\mathbf{x}^{(0)}}^\\mathrm{T}+\\cdots+\\nabla_{\\mathbf{z^{(b-1)}}}(L) {\\mathbf{x}^{(b-1)}}^\\mathrm{T}\\right].\\end{align*}$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size, reg_strength = 0.0):\n",
        "        self.weights = 0.01*np.random.randn(output_size, input_size + 1) # bias trick\n",
        "        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n",
        "        self.reg_strength = reg_strength\n",
        "        self.reg_loss = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
        "        self.output= np.dot(self.weights, self.input)\n",
        "        # Calculate the regularization loss (L2 regularization)\n",
        "        self.reg_loss = self.reg_strength * np.sum(self.weights[:, :-1] * self.weights[:, :-1])\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        ## Following is the inefficient way of calculating the backward gradient\n",
        "        weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n",
        "        for b in range(output_gradient.shape[1]):\n",
        "          weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n",
        "        weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n",
        "\n",
        "        # Add the regularization loss gradient here\n",
        "        weights_gradient += 2*self.reg_strength * np.hstack([self.weights[:, :-1], np.zeros((self.weights.shape[0], 1))])\n",
        "\n",
        "        ## Following is the efficient way of calculating the weights gradient\n",
        "        #weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
        "\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "\n",
        "        # Update weights using gradient descent step\n",
        "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
        "\n",
        "        return(input_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "---\n",
        "\n",
        "Function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKFmCaFsJhkR"
      },
      "source": [
        "---\n",
        "\n",
        "Example generation of batch indices\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k9QwikN0IYSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb74a7d1-12b1-457e-80e4-cf24eb1c3742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([26, 19, 21, 20,  2,  9, 22, 14]), array([10, 29, 18,  6, 31, 11,  4,  5]), array([ 3, 27, 23,  7, 12, 25,  0, 17]), array([15,  1, 24, 30, 28, 16, 13,  8])]\n"
          ]
        }
      ],
      "source": [
        "## Example generation of batch indices\n",
        "batch_size = 8\n",
        "batch_indices = generate_batch_indices(32, batch_size)\n",
        "print(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "---\n",
        "\n",
        "Train the 2-layer neural network (128 nodes in the hidden layer) using batch training with batch size = 100\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LGIzrN-rPuI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3767cc-7fdf-4f2f-a684-f7a2148d7769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss = 2.299389, test loss = 2.295193\n",
            "Epoch 2: train loss = 2.290846, test loss = 2.285164\n",
            "Epoch 3: train loss = 2.278840, test loss = 2.270139\n",
            "Epoch 4: train loss = 2.260246, test loss = 2.246567\n",
            "Epoch 5: train loss = 2.231139, test loss = 2.210017\n",
            "Epoch 6: train loss = 2.186766, test loss = 2.155294\n",
            "Epoch 7: train loss = 2.121940, test loss = 2.077232\n",
            "Epoch 8: train loss = 2.032109, test loss = 1.971948\n",
            "Epoch 9: train loss = 1.914963, test loss = 1.839202\n",
            "Epoch 10: train loss = 1.773412, test loss = 1.685753\n",
            "Epoch 11: train loss = 1.617639, test loss = 1.525448\n",
            "Epoch 12: train loss = 1.462395, test loss = 1.373296\n",
            "Epoch 13: train loss = 1.320055, test loss = 1.238714\n",
            "Epoch 14: train loss = 1.196626, test loss = 1.124381\n",
            "Epoch 15: train loss = 1.092628, test loss = 1.028817\n",
            "Epoch 16: train loss = 1.005702, test loss = 0.949120\n",
            "Epoch 17: train loss = 0.933012, test loss = 0.882317\n",
            "Epoch 18: train loss = 0.871836, test loss = 0.826076\n",
            "Epoch 19: train loss = 0.819985, test loss = 0.778117\n",
            "Epoch 20: train loss = 0.775672, test loss = 0.737115\n",
            "Epoch 21: train loss = 0.737471, test loss = 0.701463\n",
            "Epoch 22: train loss = 0.704264, test loss = 0.670402\n",
            "Epoch 23: train loss = 0.675230, test loss = 0.643403\n",
            "Epoch 24: train loss = 0.649672, test loss = 0.619207\n",
            "Epoch 25: train loss = 0.626930, test loss = 0.597683\n",
            "Epoch 26: train loss = 0.606678, test loss = 0.578365\n",
            "Epoch 27: train loss = 0.588434, test loss = 0.561193\n",
            "Epoch 28: train loss = 0.571934, test loss = 0.545429\n",
            "Epoch 29: train loss = 0.557065, test loss = 0.531112\n",
            "Epoch 30: train loss = 0.543437, test loss = 0.518061\n",
            "Epoch 31: train loss = 0.530947, test loss = 0.506309\n",
            "Epoch 32: train loss = 0.519523, test loss = 0.495202\n",
            "Epoch 33: train loss = 0.508975, test loss = 0.484957\n",
            "Epoch 34: train loss = 0.499224, test loss = 0.475665\n",
            "Epoch 35: train loss = 0.490192, test loss = 0.466896\n",
            "Epoch 36: train loss = 0.481799, test loss = 0.458935\n",
            "Epoch 37: train loss = 0.473970, test loss = 0.451400\n",
            "Epoch 38: train loss = 0.466677, test loss = 0.444313\n",
            "Epoch 39: train loss = 0.459858, test loss = 0.437852\n",
            "Epoch 40: train loss = 0.453481, test loss = 0.431923\n",
            "Epoch 41: train loss = 0.447483, test loss = 0.426270\n",
            "Epoch 42: train loss = 0.441846, test loss = 0.420881\n",
            "Epoch 43: train loss = 0.436555, test loss = 0.415652\n",
            "Epoch 44: train loss = 0.431539, test loss = 0.410762\n",
            "Epoch 45: train loss = 0.426823, test loss = 0.406267\n",
            "Epoch 46: train loss = 0.422338, test loss = 0.402143\n",
            "Epoch 47: train loss = 0.418098, test loss = 0.398046\n",
            "Epoch 48: train loss = 0.414089, test loss = 0.394187\n",
            "Epoch 49: train loss = 0.410225, test loss = 0.390638\n",
            "Epoch 50: train loss = 0.406633, test loss = 0.387163\n"
          ]
        }
      ],
      "source": [
        "## Train the 2-layer neural network (128 nodes in the hidden layer)\n",
        "## using batch training with batch size = 100\n",
        "learning_rate = 1e-3 # learning rate\n",
        "batch_size = 100 # batch size\n",
        "nepochs = 50 # number of epochs\n",
        "reg_strength = 0 # regularization strength\n",
        "\n",
        "# Create empty array to store training losses over each epoch\n",
        "loss_train_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "\n",
        "# Create empty array to store test losses over each epoch\n",
        "loss_test_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "\n",
        "# Define neural network architecture\n",
        "dlayer1 = Dense(num_features, 128, reg_strength) # define dense layer 1\n",
        "alayer1 = ReLU() # define activation layer 1\n",
        "dlayer2 = Dense(128, num_labels, reg_strength) # define dense layer 2\n",
        "softmax = Softmax() # define softmax activation layer\n",
        "\n",
        "# Steps: run over each batch, calculate average loss, average gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "epoch = 0\n",
        "while epoch < nepochs:\n",
        "  batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "  loss = 0\n",
        "  # Run over each batch\n",
        "  for b in range(len(batch_indices)):\n",
        "    # Forward prop starts here\n",
        "    dlayer1.forward(X_train[:, batch_indices[b]])\n",
        "    alayer1.forward(dlayer1.output)\n",
        "    dlayer2.forward(alayer1.output)\n",
        "    softmax.forward(dlayer2.output)\n",
        "\n",
        "    # Calculate training data loss\n",
        "    loss += cce(Y_train[:, batch_indices[b]], softmax.output)\n",
        "    loss += dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "    # Backward prop starts here\n",
        "    grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n",
        "    grad = softmax.backward(grad)\n",
        "    grad = dlayer2.backward(grad, learning_rate)\n",
        "    grad = alayer1.backward(grad)\n",
        "    grad = dlayer1.backward(grad, learning_rate)\n",
        "\n",
        "  # Calculate the average training loss for the current epoch\n",
        "  loss_train_epoch[epoch] = loss / len(batch_indices)\n",
        "\n",
        "  # Forward propagation for the test data\n",
        "  dlayer1.forward(X_test)\n",
        "  alayer1.forward(dlayer1.output)\n",
        "  dlayer2.forward(alayer1.output)\n",
        "  softmax.forward(dlayer2.output)\n",
        "\n",
        "  # Calculate the test data loss for the current epoch\n",
        "  loss_test_epoch[epoch] = cce(Y_test, softmax.output) + dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "  print('Epoch %d: train loss = %f, test loss = %f'%(epoch+1, loss_train_epoch[epoch], loss_test_epoch[epoch]))\n",
        "  epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Plot train and test loss as a function of epoch\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2CWdkpLpXRmz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Iv3k23SlCqGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "e4248d23-4b4c-4dfa-adc3-5a213b84d3e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot train and test loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot(loss_train_epoch, 'b', label = 'Train')\n",
        "ax.plot(loss_test_epoch, 'r', label = 'Test')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.legend()\n",
        "ax.set_title('Loss vs. Epoch for reg. strength 0.01', fontsize = 14);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Test performance on test data\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0gCpbGyNXX8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7AEbmpcKcPY"
      },
      "outputs": [],
      "source": [
        "## Forward propagate using the weights from the last epoch\n",
        "dlayer1.forward(X_test)\n",
        "alayer1.forward(dlayer1.output)\n",
        "dlayer2.forward(alayer1.output)\n",
        "softmax.forward(dlayer2.output)\n",
        "\n",
        "# Predict the output labels for the test data\n",
        "ypred = np.argmax(softmax.output.T, axis = 1)\n",
        "print(ypred)\n",
        "ytrue = np.argmax(Y_test.T, axis = 1)\n",
        "print(ytrue)\n",
        "np.mean(ytrue == ypred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "X3FFv71B-fX9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(ytrue, ypred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAhtnJmO-lt0",
        "outputId": "cc72505e-909e-4fae-852b-4e25de9c6fad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 966,    0,    2,    6,    0,    0,    3,    1,    2,    0],\n",
              "       [   0, 1123,    4,    4,    0,    0,    1,    1,    2,    0],\n",
              "       [  99,   70,  695,   66,    9,    0,   52,   27,    7,    7],\n",
              "       [  75,   32,   26,  816,    0,    0,    2,   35,   17,    7],\n",
              "       [  24,   18,    4,    0,  324,    0,   37,   18,    5,  552],\n",
              "       [ 364,   38,   31,  259,    5,    0,   28,   72,   43,   52],\n",
              "       [ 158,   15,   58,   10,   14,    0,  697,    1,    5,    0],\n",
              "       [  20,   62,   16,    1,    0,    0,    0,  908,    5,   16],\n",
              "       [  87,   91,   65,  238,    2,    0,   16,   42,  368,   65],\n",
              "       [  34,   23,    2,   11,   35,    0,    6,  230,    2,  666]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}