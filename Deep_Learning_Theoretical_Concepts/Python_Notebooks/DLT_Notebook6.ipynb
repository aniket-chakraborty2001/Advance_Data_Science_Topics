{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5dEgRpy3952M"},"source":["## Load libraries\n","import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.style.use('dark_background')\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.set_printoptions(precision=2)"],"metadata":{"id":"G9W_1_v_6yq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"4T7eUtw7Mh0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"id":"Q1e2N5S8MlCU","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"f6299319-9ade-4fed-e6ef-6e3f334867b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.15.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Generate artificial data with 10 samples, 5 features per sample\n","# and 3 output classes\n","num_samples = 10 # number of samples\n","num_features = 5 # number of features (a.k.a. dimensionality)\n","num_labels = 3 # number of output labels\n","# Data matrix (each column = single sample)\n","X = np.random.choice(np.arange(3, 10), size = (num_features, num_samples), replace = True)\n","# Class labels\n","y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n","print(X)\n","print('------')\n","print(y)\n","print('------')\n","# One-hot encode class labels\n","y = tf.keras.utils.to_categorical(y).T\n","print(y)"],"metadata":{"id":"B4EjOi-OM4Gp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"09b1f7cf-5411-4ccb-bf18-89d050cbffe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[5 4 5 3 3 5 8 5 4 8]\n"," [8 8 8 7 9 4 6 6 8 9]\n"," [3 4 8 4 3 8 5 3 7 4]\n"," [7 9 9 4 9 3 7 5 5 3]\n"," [7 9 4 6 4 6 6 9 8 5]]\n","------\n","[1 0 1 2 0 0 1 2 2 0]\n","------\n","[[0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n"," [1. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 1. 1. 0.]]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","A generic layer class with forward and backward methods\n","\n","----"],"metadata":{"id":"IrXipxwrJ0_8"}},{"cell_type":"code","source":["class Layer:\n","  def __init__(self):\n","    self.input = None\n","    self.output = None\n","\n","  def forward(self, input):\n","    pass\n","\n","  def backward(self, output_gradient, learning_rate):\n","    pass"],"metadata":{"id":"N4pKUhCyMrWm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The softmax classifier steps for a generic sample $\\mathbf{x}$ with (one-hot encoded) true label $\\mathbf{y}$ (3 possible categories) using a randomly initialized weights matrix (with bias absorbed as its last last column):\n","\n","1. Calculate raw scores vector for a generic sample $\\mathbf{x}$  (bias feature added): $$\\mathbf{z} = \\mathbf{Wx}.$$\n","2. Calculate softmax probabilities (that is, softmax-activate the raw scores) $$\\mathbf{a} = \\text{softmax}(\\mathbf{z})\\Rightarrow\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}= \\text{softmax}\\left(\\begin{bmatrix}z_0\\\\z_1\\\\z_2\\end{bmatrix}\\right)=\\begin{bmatrix}\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\end{bmatrix}$$\n","3. Softmax loss for this sample is (where output label $y$ is not yet one-hot encoded)\n","$$\\begin{align*}L &=  -\\log([\\mathbf{a}]_y) \\\\&= -\\log\\left(\\left[\\text{softmax}(\\mathbf{z})\\right]_y\\right)\\\\ &= -\\log\\left(\\left[\\text{softmax}(\\mathbf{Wx})\\right]_y\\right).\\end{align*}$$\n","4. Predicted probability vector that the sample belongs to each one of the output categories is given a new name $$\\hat{\\mathbf{y}} = \\mathbf{a}.$$\n","5. One-hot encoding the output label $$\\underbrace{y\\rightarrow\\mathbf{y}}_{\\text{e.g.}\\,2\\,\\rightarrow\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}}$$ results in the following representation for the softmax loss for the sample which is also referred to as the categorical crossentropy (CCE) loss:\n","$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)=-\\mathbf{y}^\\mathrm{T}\\log\\left(\\hat{\\mathbf{y}}\\right)\\end{align*}.$$\n","5. Calculate the gradient of the loss for the sample w.r.t. weights by following the computation graph from top to bottom (that is, backward):\n","$$\\begin{align*} L\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}} &= \\mathbf{a}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W},\\mathbf{x}\\end{align*}$$\n","\n","$$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\hat{\\mathbf{y}})\\times\\nabla_{\\hat{\\mathbf{y}}}(L)\\\\&= \\underbrace{\\nabla_\\mathbf{W}(\\mathbf{z})}_\\text{first term} \\times\\underbrace{\\nabla_\\mathbf{z}(\\mathbf{a})}_\\text{second to last term}\\times\\underbrace{\\nabla_\\hat{\\mathbf{y}}(L)}_\\text{last term}.\\end{align*}$$Gradient descent step:\n","$$\\mathbf{W} = \\mathbf{W}+\\alpha\\times\\left(-\\nabla_\\mathbf{W}(L)\\right).$$\n","7. Now focus on the last term $\\nabla_\\hat{\\mathbf{y}}(L)$:\n","$$\\begin{align*}\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n","8. Now focus on the second to last term $\\nabla_\\mathbf{z}(\\mathbf{a})$:\n","$$\\begin{align*}\\nabla_\\mathbf{z}(\\mathbf{a}) &= \\nabla_\\mathbf{z}\\left(\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}\\right)\\\\ &= \\begin{bmatrix}\\nabla_\\mathbf{z}(a_0)&\\nabla_\\mathbf{z}(a_1)&\\nabla_\\mathbf{z}(a_2)\\end{bmatrix} \\\\&= \\begin{bmatrix}\\nabla_{z_0}(a_0)&\\nabla_{z_0}(a_1)&\\nabla_{z_0}(a_2)\\\\\\nabla_{z_1}(a_0)&\\nabla_{z_1}(a_1)&\\nabla_{z_1}(a_2)\\\\\\nabla_{z_2}(a_0)&\\nabla_{z_2}(a_1)&\\nabla_{z_2}(a_2)\\end{bmatrix}\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}.\\end{align*}$$\n","9. Now focus on the last term $\\nabla_\\mathbf{W}(\\mathbf{z}) = \\nabla_\\mathbf{W}(\\mathbf{Wx})$:\n","\n","![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103155&authkey=%21AMH79mXBdb_raAA&width=660)\n","\n","The full gradient can be written as $\\nabla_\\mathbf{W}(L)=$\n","\n","![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103156&authkey=%21AIdyOQ3a-er-7-A&width=660)\n","\n","$$\\begin{align*}=\\begin{bmatrix}a_1(1-a_1)&-a_2a_1&-a_3a_1\\\\-a_1a_2&a_2(1-a_2)&-a_3a_2\\\\-a_1a_3&-a_2a_3&a_3(1-a_3)\\end{bmatrix}\\times\\begin{bmatrix}-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\\\-y_3/\\hat{y}_3\\end{bmatrix}\\mathbf{x}^\\mathrm{T}.\\end{align*}$$\n","\n","\n","---"],"metadata":{"id":"vdLfiQSlOSUU"}},{"cell_type":"markdown","source":["---\n","\n","CCE loss and its gradient\n","\n","$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right) = -\\mathbf{y}^\\mathrm{T}\\log\\left(\\hat{\\mathbf{y}}\\right)\\\\\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n","\n","\n","---"],"metadata":{"id":"b9YGwzbz72CZ"}},{"cell_type":"code","source":["## Define the loss function and its gradient\n","def cce(y, yhat):\n","  return(-np.sum(y*np.log(yhat)))\n","  #return(np.dot(-y, np.log(yhat)))\n","\n","def cce_gradient(y, yhat):\n","  return(-y/yhat)\n","\n","# TensorFlow in-built function for categorical crossentropy loss\n","#cce = tf.keras.losses.CategoricalCrossentropy()"],"metadata":{"id":"hdXSGW2s7zKd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Softmax activation layer class\n","$$\\begin{align*}\\text{forward:}\\ \\mathbf{a} &=\\text{softmax}(\\mathbf{z}),\\\\\\text{backward:}\\ \\nabla_\\mathbf{z}(L) &= \\nabla_{\\mathbf{z}}(\\mathbf{a})\\times\\nabla_{\\mathbf{a}}(L) = \\nabla_{\\mathbf{z}}(\\mathbf{a})\\times\\nabla_{\\hat{\\mathbf{y}}}(L)\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}\\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n","\n","\n","---"],"metadata":{"id":"smgXLg9p65HV"}},{"cell_type":"code","source":["z = np.array([-1., 2., 3.])\n","tf.nn.softmax(z).numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaKB-TYYlMtq","outputId":"5fad7f42-cd42-40e5-e081-130ab7913dec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.01, 0.27, 0.72])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["## Softmax activation layer class\n","class Softmax(Layer):\n","  def forward(self, input):\n","    self.output = tf.nn.softmax(input).numpy()\n","\n","  def backward(self, output_gradient, learning_rate = None):\n","    return(np.dot((np.identity(np.size(self.output))-self.output.T) * self.output, output_gradient))"],"metadata":{"id":"4x1Xn3AbJlNy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Dense layer class\n","\n","$$\\begin{align*}\\text{forward:}\\ \\mathbf{z}&=\\mathbf{Wx}\\\\\\text{backward:}\\ \\nabla_\\mathbf{W}(L)&=\\nabla_{\\mathbf{W}}(\\mathbf{z})\\times\\nabla_{\\mathbf{z}}(L)\\\\&=\\nabla_{\\mathbf{z}}(L)\\mathbf{x}^\\mathrm{T}.\\end{align*}$$\n","\n","---"],"metadata":{"id":"XkPFfd1U68dj"}},{"cell_type":"code","source":["np.empty((3, 5+1), dtype = np.float64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIb-EUxfh1f4","outputId":"f861d009-1d9d-4b75-a15b-fab6f067eabc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 4.74e-310,  0.00e+000,  6.54e-310,  6.54e-310,  1.88e+079,\n","         1.27e+084],\n","       [ 2.41e+050,  5.30e+093,  1.88e+079, -2.89e+127,  2.41e+050,\n","         6.41e+117],\n","       [ 1.85e+079, -2.89e+127, -7.66e+136, -3.29e+146,  6.70e-287,\n","        -5.49e+303]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["np.random.randn(3, 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-_GhHiFiUrm","outputId":"5a0fc031-90c1-4ee6-e394-095c114dac78"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.32, -0.74, -0.24,  2.49, -1.34],\n","       [-0.18, -1.  , -2.01,  0.19,  1.36],\n","       [ 0.55, -1.25, -0.11,  0.08,  0.47]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["## Dense layer class\n","class Dense(Layer):\n","    def __init__(self, input_size, output_size):\n","        self.weights = np.empty((output_size, input_size+1), dtype = np.float64) # bias trick\n","        # Initializing the weights part of the weights matrix\n","        self.weights[:, :-1] = 0.01 * np.random.randn(output_size, input_size)\n","        self.weights[:, -1] = 0.01 # Set all bias values to the same nonzero constant\n","\n","    def forward(self, input):\n","        self.input = np.append(input, 1) # adding the bias feature 1\n","        self.output = np.dot(self.weights, self.input)\n","\n","    def backward(self, output_gradient, learning_rate):\n","        weights_gradient = np.dot(output_gradient.reshape(-1, 1), self.input.reshape(-1, 1).T)\n","        input_gradient = np.dot(self.weights.T, output_gradient)\n","        self.weights = self.weights + learning_rate * (-weights_gradient)\n","        return input_gradient"],"metadata":{"id":"8ctXhZYCTmHK","executionInfo":{"status":"error","timestamp":1709786445797,"user_tz":-330,"elapsed":34,"user":{"displayName":"Aniket Chakraborty","userId":"02359869981756837222"}},"outputId":"e919571b-cfc6-4e12-ead5-f886c64f3cdc","colab":{"base_uri":"https://localhost:8080/","height":211}},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Layer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4ca26fe6ae1b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Dense layer class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bias trick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Initializing the weights part of the weights matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Layer' is not defined"]}]},{"cell_type":"code","source":["# Forward and backward propagation for the 0th sample\n","learning_rate = 1e-02\n","\n","# Define the neural network architecture\n","dlayer = Dense(num_features, num_labels) # define the dense layer\n","#print(dlayer.weights)\n","softmax = Softmax() # define the softmax activation layer\n","print(dlayer.weights)\n","\n","# Forward propagation starts here\n","dlayer.forward(X[:, 0]) # forward prop for the 0th sample\n","softmax.forward(dlayer.output)\n","loss = cce(y[:, 0], softmax.output)\n","#print(loss)\n","\n","# Backward propagation starts here\n","grad = cce_gradient(y[:, 0], softmax.output)\n","grad = softmax.backward(grad)\n","grad = dlayer.backward(grad, learning_rate)\n","print('-----')\n","print(dlayer.weights)"],"metadata":{"id":"NPVu8RC5Tvmy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af8d6050-93fa-4511-d660-b035ec1f344a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.01  0.   -0.    0.01  0.01  0.01]\n"," [ 0.   -0.    0.    0.01  0.01  0.01]\n"," [-0.02  0.02  0.01  0.02 -0.    0.01]]\n","-----\n","[[-0.02 -0.02 -0.01 -0.01 -0.01  0.01]\n"," [ 0.04  0.05  0.02  0.05  0.06  0.02]\n"," [-0.03 -0.01  0.   -0.   -0.02  0.01]]\n"]}]},{"cell_type":"code","source":["## Train the 0-layer neural network using batch training with batch size = 1\n","\n","# Steps: run over each sample, calculate loss, gradient of loss,\n","# and update weights.\n","\n","# Forward and backward propagation for the 0th sample\n","learning_rate = 1e-02\n","\n","# Define the neural network architecture\n","dlayer = Dense(num_features, num_labels) # define the dense layer\n","#print(dlayer.weights)\n","softmax = Softmax() # define the softmax activation layer\n","\n","loss = 0.0\n","\n","for i in range(X.shape[1]):\n","  # Forward propagation starts here\n","  dlayer.forward(X[:, i]) # forward prop for the 0th sample\n","  softmax.forward(dlayer.output)\n","  loss = cce(y[:, i], softmax.output)\n","\n","  # Backward propagation starts here\n","  grad = cce_gradient(y[:, i], softmax.output)\n","  grad = softmax.backward(grad)\n","  grad = dlayer.backward(grad, learning_rate)\n","\n","  print('Sample %d, loss = %f'%(i, loss))\n","\n"],"metadata":{"id":"LGIzrN-rPuI4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce3b48e2-754d-476e-d41e-4cee7769d2d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample 0, loss = 0.849144\n","Sample 1, loss = 2.785495\n","Sample 2, loss = 0.739394\n","Sample 3, loss = 3.617643\n","Sample 4, loss = 2.303721\n","Sample 5, loss = 1.050931\n","Sample 6, loss = 1.376384\n","Sample 7, loss = 4.820576\n","Sample 8, loss = 3.365870\n","Sample 9, loss = 1.573081\n"]}]}]}