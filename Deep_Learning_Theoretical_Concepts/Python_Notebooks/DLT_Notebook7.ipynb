{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5dEgRpy3952M"},"outputs":[],"source":["## Load libraries\n","import pandas as pd\n","import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","from keras.datasets import mnist\n","plt.style.use('dark_background')\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9W_1_v_6yq7"},"outputs":[],"source":["np.set_printoptions(precision=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4T7eUtw7Mh0z"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1e2N5S8MlCU"},"outputs":[],"source":["tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"16BpVeIWIOks"},"source":["---\n","\n","Load MNIST Data\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5kaKFKSIQgu"},"outputs":[],"source":["## Load MNIST data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","X_train = X_train.transpose(1, 2, 0)\n","X_test = X_test.transpose(1, 2, 0)\n","X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n","X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n","\n","num_labels = len(np.unique(y_train))\n","num_features = X_train.shape[0]\n","num_samples = X_train.shape[1]\n","\n","# One-hot encode class labels\n","Y_train = tf.keras.utils.to_categorical(y_train).T\n","Y_test = tf.keras.utils.to_categorical(y_test).T\n","\n","\n","# Normalize the samples (images)\n","xmax = np.amax(X_train)\n","xmin = np.amin(X_train)\n","X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n","X_test = (X_test - xmin)/(xmax - xmin)\n","\n","print('MNIST set')\n","print('---------------------')\n","print('Number of training samples = %d'%(num_samples))\n","print('Number of features = %d'%(num_features))\n","print('Number of output labels = %d'%(num_labels))"]},{"cell_type":"markdown","metadata":{"id":"IrXipxwrJ0_8"},"source":["---\n","\n","A generic layer class with forward and backward methods\n","\n","----"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4pKUhCyMrWm"},"outputs":[],"source":["class Layer:\n","  def __init__(self):\n","    self.input = None\n","    self.output = None\n","\n","  def forward(self, input):\n","    pass\n","\n","  def backward(self, output_gradient, learning_rate):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"b9YGwzbz72CZ"},"source":["---\n","\n","CCE loss and its gradient for the batch samples:\n","\n","$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+\\cdots+L_{b-1}\\right]\\\\&=\\frac{1}{b}\\left[-{\\mathbf{y}^{(0)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(0)}\\right)+\\cdots+-{\\mathbf{y}^{(b-1)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\right].\\end{align*}$$\n","\n","$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)&\\ldots&\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\end{bmatrix}=\\begin{bmatrix}-y_0^{(0)}/\\hat{y}_0^{(0)}&\\cdots&-y_0^{(b-1)}/\\hat{y}_0^{(b-1)}\\\\-y_1^{(0)}/\\hat{y}_1^{(0)}&\\ldots&-y_1^{(b-1)}/\\hat{y}_1^{(b-1)}\\\\-y_2^{(0)}/\\hat{y}_2^{(0)}&\\cdots&-y_2^{(b-1)}/\\hat{y}_2^{(b-1)}\\\\\\vdots\\\\-y_9^{(0)}/\\hat{y}_9^{(0)}&\\cdots&-y_9^{(b-1)}/\\hat{y}_9^{(b-1)}\\end{bmatrix}.\\end{align*}$$\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdXSGW2s7zKd"},"outputs":[],"source":["## Define the CCE loss function and its gradient\n","def cce(Y, Yhat):\n","  return(np.mean(np.sum(-Y*np.log(Yhat), axis = 0)))\n","  #TensorFlow in-built function for categorical crossentropy loss\n","  #cce = tf.keras.losses.CategoricalCrossentropy()\n","  #return(cce(Y, Yhat).numpy())\n","\n","def cce_gradient(Y, Yhat):\n","  return(-Y/Yhat)"]},{"cell_type":"markdown","source":["Generic activation layer class:\n","\n","**Forward**:\n","$$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\sigma\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\sigma\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\sigma(\\mathbf{Z}).\\end{align*}$$\n","\n","**Backward**:\n","$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}(L_0)&\\ldots&\\nabla_{\\mathbf{z}^{(b-1)}}(L_{b-1})\\end{bmatrix} &= \\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\cdots&\\nabla_{\\mathbf{z}^{(b-1)}}\\left({\\mathbf{a}}^{(b-1)}\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}\\\\&=\\begin{bmatrix}\\text{diag}\\left(\\nabla_{\\mathbf{z}^{(0)}}\\left(\\sigma\\left(\\mathbf{z}^{(0)}\\right)\\right)\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\ldots&\\text{diag}\\left(\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\sigma\\left(\\mathbf{z}^{(b-1)}\\right)\\right)\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}.\\end{align*}$$\n","\n"],"metadata":{"id":"ba6WoPMRwGXz"}},{"cell_type":"code","source":["class Activation(Layer):\n","    def __init__(self, activation, activation_gradient):\n","        self.activation = activation\n","        self.activation_gradient = activation_gradient\n","\n","    def forward(self, input):\n","        self.input = input\n","        self.output = self.activation(self.input)\n","        return(self.output)\n","\n","    def backward(self, output_gradient, learning_rate = None):\n","        return(output_gradient[:-1, :] * self.activation_gradient(self.input))"],"metadata":{"id":"C21FcWIEwGCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Specific activation layer classes:"],"metadata":{"id":"JheGWSoKxYWu"}},{"cell_type":"code","source":["class Sigmoid(Activation):\n","    def __init__(self):\n","        def sigmoid(z):\n","            return 1 / (1 + np.exp(-z))\n","\n","        def sigmoid_gradient(z):\n","            a = sigmoid(z)\n","            return a * (1 - a)\n","\n","        super().__init__(sigmoid, sigmoid_gradient)\n","\n","class Tanh(Activation):\n","    def __init__(self):\n","        def tanh(z):\n","            return np.tanh(z)\n","\n","        def tanh_gradient(z):\n","            return 1 - np.tanh(z) ** 2\n","\n","        super().__init__(tanh, tanh_gradient)\n","\n","class ReLU(Activation):\n","    def __init__(self):\n","        def relu(z):\n","            return z * (z > 0)\n","\n","        def relu_gradient(z):\n","            return 1. * (z > 0)\n","\n","        super().__init__(relu, relu_gradient)"],"metadata":{"id":"PQ5ybz_Yxbef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"smgXLg9p65HV"},"source":["---\n","\n","Softmax activation layer class:\n","\n","**Forward**:\n","$$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\text{softmax}\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\text{softmax}\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\text{softmax}(\\mathbf{Z}).\\end{align*}$$\n","\n","**Backward**:\n","$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}(L_0)&\\ldots&\\nabla_{\\mathbf{z}^{(b-1)}}(L_{b-1})\\end{bmatrix} &= \\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\cdots&\\nabla_{\\mathbf{z}^{(b-1)}}\\left({\\mathbf{a}}^{(b-1)}\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}\\end{align*}$$\n","\n","![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103299&authkey=%21AIPPR63BJ3UybA8&width=928&height=99)\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4x1Xn3AbJlNy"},"outputs":[],"source":["## Softmax activation layer class\n","class Softmax(Layer):\n","  def forward(self, input):\n","    self.output = tf.nn.softmax(input, axis = 0).numpy()\n","\n","  def backward(self, output_gradient, learning_rate = None):\n","    ## Following is the inefficient way of calculating the backward gradient\n","    #softmax_gradient = np.empty((self.output.shape[0], output_gradient.shape[1]), dtype = np.float64)\n","    #for b in range(softmax_gradient.shape[1]):\n","    #  softmax_gradient[:, b] = np.dot((np.identity(self.output.shape[0])-np.atleast_2d(self.output[:, b])) * np.atleast_2d(self.output[:, b]).T, output_gradient[:, b])\n","    #return(softmax_gradient)\n","\n","    ## Following is the efficient of calculating the backward gradient\n","    T = np.transpose(np.identity(self.output.shape[0]) - np.atleast_2d(self.output).T[:, np.newaxis, :], (2, 1, 0)) * np.atleast_2d(self.output)\n","    return(np.einsum('jik, ik -> jk', T, output_gradient))"]},{"cell_type":"markdown","metadata":{"id":"XkPFfd1U68dj"},"source":["---\n","\n","Dense layer class:\n","\n","**Forward**:\n","$$$$\\begin{align*}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix} &= \\mathbf{W}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix}\\\\&=\\begin{bmatrix}\\mathbf{W}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{W}\\mathbf{z}^{(b-1)}\\end{bmatrix}\\\\\\Rightarrow \\mathbf{Z} &= \\mathbf{WX}.\\end{align*}$$$$\n","\n","**Backward**:\n","$$\\begin{align*}\\nabla_\\mathbf{W}(L)&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{W}}(\\mathbf{z}^{(0)})\\times\\nabla_{\\mathbf{z^{(0)}}}(L) +\\cdots+ \\nabla_{\\mathbf{W}}(\\mathbf{z}^{(b-1)})\\times\\nabla_{\\mathbf{z^{(b-1)}}}(L)\\right]\\\\&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{z^{(0)}}}(L){\\mathbf{x}^{(0)}}^\\mathrm{T}+\\cdots+\\nabla_{\\mathbf{z^{(b-1)}}}(L) {\\mathbf{x}^{(b-1)}}^\\mathrm{T}\\right].\\end{align*}$$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ctXhZYCTmHK"},"outputs":[],"source":["## Dense layer class\n","class Dense(Layer):\n","    def __init__(self, input_size, output_size, reg_strength):\n","        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n","        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n","        self.reg_strength = reg_strength # regularization strength\n","        self.reg_loss = None # regularization loss\n","\n","    def forward(self, input):\n","        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n","        self.output= np.dot(self.weights, self.input)\n","        # Calculate regularization loss\n","        self.reg_loss = self.reg_strength * np.sum(self.weights[:, :-1] * self.weights[:, :-1])\n","\n","    def backward(self, output_gradient, learning_rate):\n","        ## Following is the inefficient way of calculating the weights gradient\n","        #weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n","        #for b in range(output_gradient.shape[1]):\n","        #  weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n","        # Training loss gradient\n","        #weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n","\n","        ## Following is the efficient way of calculating the weights gradient\n","        weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n","\n","        # Add regularization loss gradient to training loss gradient\n","        weights_gradient += self.reg_strength * 2 * np.hstack([self.weights[:, :-1], np.zeros((self.weights.shape[0], 1))])\n","\n","        ## Calculate backward gradient on the input side\n","        input_gradient = np.dot(self.weights.T, output_gradient)\n","\n","        # Update weights using gradient descent step\n","        self.weights = self.weights + learning_rate * (-weights_gradient)\n","\n","        return(input_gradient)"]},{"cell_type":"markdown","metadata":{"id":"2W1howeOJegI"},"source":["---\n","\n","Function to generate sample indices for batch processing according to batch size\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHyjEf22IRpc"},"outputs":[],"source":["## Function to generate sample indices for batch processing according to batch size\n","def generate_batch_indices(num_samples, batch_size):\n","  # Reorder sample indices\n","  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n","  # Generate batch indices for batch processing\n","  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n","  return(batch_indices)"]},{"cell_type":"markdown","metadata":{"id":"fI_Gms9fJqbs"},"source":["---\n","\n","Train the 1 hidden layer neural network using batch training with batch size = 16\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGIzrN-rPuI4"},"outputs":[],"source":["## Train the 1 hidden layer neural network using batch training with batch size = 100\n","learning_rate = 1e-1 # learning rate\n","batch_size = 100 # batch size\n","nepochs = 100 # number of epochs\n","reg_strength = 1e-02 # regularization strength\n","loss_train_epoch = np.empty(nepochs, dtype = np.float64) # create empty array to store train losses over each epoch\n","loss_test_epoch = np.empty(nepochs, dtype = np.float64) # create empty array to store test losses over each epoch\n","\n","# Neural network architecture\n","dlayer1 = Dense(num_features, 128, reg_strength) # define dense layer 1\n","alayer1 = ReLU() # ReLU activation layer 1\n","dlayer2 = Dense(128, num_labels, reg_strength) # define dense layer 2\n","softmax = Softmax() # define softmax activation layer\n","\n","# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n","# and update weights.\n","\n","epoch = 0\n","while epoch < nepochs:\n","  batch_indices = generate_batch_indices(num_samples, batch_size)\n","  loss = 0\n","  for b in range(len(batch_indices)):\n","    # Forward prop for train data starts here\n","    dlayer1.forward(X_train[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n","    alayer1.forward(dlayer1.output) # forward prop activation layer 1\n","    dlayer2.forward(alayer1.output) # forward prop dense layer 2\n","    softmax.forward(dlayer2.output) # Softmax activate\n","    loss += cce(Y_train[:, batch_indices[b]], softmax.output) # calculate training loss\n","    loss += dlayer1.reg_loss + dlayer2.reg_loss # add regularization loss for each dense layer\n","    # Backward prop for train data starts here\n","    grad = cce_gradient(Y_train[:, batch_indices[b]], softmax.output)\n","    grad = softmax.backward(grad)\n","    grad = dlayer2.backward(grad, learning_rate)\n","    grad = alayer1.backward(grad)\n","    grad = dlayer1.backward(grad, learning_rate)\n","  # Calculate average training loss over epoch\n","  loss_train_epoch[epoch] = loss/len(batch_indices)\n","\n","  # Forward prop for test data starts here\n","  dlayer1.forward(X_test)\n","  alayer1.forward(dlayer1.output)\n","  dlayer2.forward(alayer1.output)\n","  softmax.forward(dlayer2.output)\n","  # Calculate test loss plus regularization loss for each dense layer\n","  loss_test_epoch[epoch] = cce(Y_test, softmax.output) + dlayer1.reg_loss + dlayer2.reg_loss\n","\n","  print('Epoch %d: training loss = %f, test loss = %f'%(epoch+1, loss_train_epoch[epoch], loss_test_epoch[epoch]))\n","  epoch = epoch + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iv3k23SlCqGf"},"outputs":[],"source":["# Plot train and test loss as a function of epoch:\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","fig.tight_layout(pad = 4.0)\n","ax.plot(loss_train_epoch, 'b', label = 'Train')\n","ax.plot(loss_test_epoch, 'r', label = 'Test')\n","ax.set_xlabel('Epoch', fontsize = 12)\n","ax.set_ylabel('Loss value', fontsize = 12)\n","ax.legend()\n","ax.set_title('Loss vs. Epoch for reg. strength 0.01', fontsize = 14)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7AEbmpcKcPY"},"outputs":[],"source":["dlayer1.forward(X_test)\n","alayer1.forward(dlayer1.output)\n","dlayer2.forward(alayer1.output)\n","softmax.forward(dlayer2.output)\n","ypred = np.argmax(softmax.output.T, axis = 1)\n","print(ypred)\n","ytrue = np.argmax(Y_test.T, axis = 1)\n","print(ytrue)\n","np.mean(ytrue == ypred)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}