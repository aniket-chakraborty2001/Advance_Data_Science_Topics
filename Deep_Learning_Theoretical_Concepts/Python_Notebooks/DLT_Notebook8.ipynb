{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kEOvx-Ddgu3J26BFDHbCSrPNlpUjFOEu","timestamp":1706442563996}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5dEgRpy3952M"},"source":["## Load libraries\n","import pandas as pd\n","import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","from keras.datasets import mnist\n","plt.style.use('dark_background')\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.set_printoptions(precision=2)"],"metadata":{"id":"G9W_1_v_6yq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"4T7eUtw7Mh0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"id":"Q1e2N5S8MlCU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Load MNIST Data\n","\n","---"],"metadata":{"id":"16BpVeIWIOks"}},{"cell_type":"code","source":["## Load MNIST data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","X_train = X_train.transpose(1, 2, 0)\n","X_test = X_test.transpose(1, 2, 0)\n","X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n","X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n","\n","num_labels = len(np.unique(y_train))\n","num_features = X_train.shape[0]\n","num_samples = X_train.shape[1]\n","\n","# One-hot encode class labels\n","Y_train = tf.keras.utils.to_categorical(y_train).T\n","Y_test = tf.keras.utils.to_categorical(y_test).T\n","\n","\n","# Normalize the samples (images)\n","xmax = np.amax(X_train)\n","xmin = np.amin(X_train)\n","X_train = (X_train - xmin) / (xmax - xmin) # all train features turn into a number between 0 and 1\n","X_test = (X_test - xmin)/(xmax - xmin)\n","\n","print('MNIST set')\n","print('---------------------')\n","print('Number of training samples = %d'%(num_samples))\n","print('Number of features = %d'%(num_features))\n","print('Number of output labels = %d'%(num_labels))"],"metadata":{"id":"E5kaKFKSIQgu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","A generic layer class with forward and backward methods\n","\n","----"],"metadata":{"id":"IrXipxwrJ0_8"}},{"cell_type":"code","source":["class Layer:\n","  def __init__(self):\n","    self.input = None\n","    self.output = None\n","\n","  def forward(self, input):\n","    pass\n","\n","  def backward(self, output_gradient, learning_rate):\n","    pass"],"metadata":{"id":"N4pKUhCyMrWm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","The softmax classifier steps for a batch of comprising $b$ samples represented as the $785\\times b$-matrix (784 pixel values plus the bias feature absorbed as its last row) $$\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}^{(0)},\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(b-1)}\\end{bmatrix}$$ with one-hot encoded true labels represented as the $10\\times b$-matrix (10 possible categories) $$\\mathbf{Y}=\\begin{bmatrix}\\mathbf{y}^{(0)}&\\ldots&\\mathbf{y}^{(b-1)}\\end{bmatrix}$$ using a randomly initialized $10\\times785$-weights matrix $\\mathbf{W}$:\n","\n","1. Calculate $10\\times b$-raw scores matrix : $$\\begin{align*}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix} &= \\mathbf{W}\\begin{bmatrix}\\mathbf{x}^{(0)}&\\ldots&\\mathbf{x}^{(b-1)}\\ldots\\end{bmatrix}\\\\&=\\begin{bmatrix}\\mathbf{W}\\mathbf{x}^{(0)}&\\ldots&\\mathbf{W}\\mathbf{x}^{(b-1)}\\end{bmatrix}\\\\\\Rightarrow \\mathbf{Z} &= \\mathbf{WX}.\\end{align*}$$\n","2. Calculate $10\\times b$-softmax predicted probabilities matrix: $$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\text{softmax}\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\text{softmax}\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\text{softmax}(\\mathbf{Z}).\\end{align*}$$\n","3. Predicted probability matrix get a new name: $\\hat{\\mathbf{Y}} = \\mathbf{A}.$\n","4. The crossentropy (CCE) loss for the $i$th sample is $$L_i = \\sum_{k=0}^9-y^{(i)}_k\\log\\left(\\hat{y}^{(i)}_k\\right) = -{\\mathbf{y}^{(i)}}^\\mathrm{T}\\log\\left(\\mathbf{y}^{(i)}\\right)$$ which leads to the average crossentropy (CCE) batch loss for the batch as:\n","$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+\\cdots+L_{b-1}\\right]\\\\&\\frac{1}{b}\\left[\\sum_{k=0}^9-y^{(0)}_k\\log\\left(\\hat{y}^{(0)}_k\\right)+\\cdots+\\sum_{k=0}^9-y^{(b-1)}_k\\log\\left(\\hat{y}^{(b-1)}_k\\right)\\right]\\\\&=\\frac{1}{b}\\left[-{\\mathbf{y}^{(0)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(0)}\\right)+\\cdots+-{\\mathbf{y}^{(b-1)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\right].\\end{align*}$$\n","5. The computational graph for the samples in the batch are presented below:\n","\n","$\\hspace{1.5in}\\begin{align*}L_0\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(0)} &= \\mathbf{a}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(0)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\\qquad\\cdots\\qquad$$\\begin{align*} L_{b-1}\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}}^{(b-1)} &= \\mathbf{a}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}^{(b-1)}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$\n","6. Calculate the gradient of the average batch loss w.r.t. weights as: $$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\frac{1}{b}\\left[\\nabla_\\mathbf{W}\\left(L_0\\right)+\\cdots+\\nabla_\\mathbf{W}\\left(L_{b-1}\\right)\\right]\\\\&= \\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left(\\hat{\\mathbf{y}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\text{sample}\\,0}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\text{sample}\\,b-1}\\right)\\\\&=\\frac{1}{b}\\left(\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(0)}\\right) \\times\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)\\right]}_{\\text{sample}\\,0}+\\cdots+\\underbrace{\\left[\\nabla_\\mathbf{W}\\left(\\mathbf{z}^{(b-1)}\\right) \\times\\nabla_{\\mathbf{z}^{(b-1)}}\\left({\\mathbf{a}}^{(b-1)}\\right)\\times\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\right]}_{\\text{sample}\\,b-1}\\right).\\end{align*}$$\n","10. The full gradient can be written as\n","\n","![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103292&authkey=%21AMoosVj6GqUSvpc&width=660)\n","\n","\n","---"],"metadata":{"id":"vdLfiQSlOSUU"}},{"cell_type":"markdown","source":["---\n","\n","CCE loss and its gradient for the batch samples:\n","\n","$$\\begin{align*}L &=\\frac{1}{b}\\left[L_0+\\cdots+L_{b-1}\\right]\\\\&=\\frac{1}{b}\\left[\\sum_{k=0}^9-y^{(0)}_k\\log\\left(\\hat{y}^{(0)}_k\\right)+\\cdots+\\sum_{k=0}^9-y^{(b-1)}_k\\log\\left(\\hat{y}^{(b-1)}_k\\right)\\right]\\\\&=\\frac{1}{b}\\left[-{\\mathbf{y}^{(0)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(0)}\\right)+\\cdots+-{\\mathbf{y}^{(b-1)}}^{\\mathrm{T}}\\log\\left(\\hat{\\mathbf{y}}^{(b-1)}\\right)\\right].\\end{align*}$$\n","\n","$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\hat{\\mathbf{y}}^{(0)}}(L_0)&\\ldots&\\nabla_{\\hat{\\mathbf{y}}^{(b-1)}}(L_{b-1})\\end{bmatrix}=\\begin{bmatrix}-y_0^{(0)}/\\hat{y}_0^{(0)}&\\cdots&-y_0^{(0)}/\\hat{y}_0^{(b-1)}\\\\-y_1^{(0)}/\\hat{y}_1^{(0)}&\\ldots&-y_1^{(b-1)}/\\hat{y}_1^{(b-1)}\\\\-y_2^{(0)}/\\hat{y}_2^{(0)}&\\cdots&-y_2^{(b-1)}/\\hat{y}_2^{(b-1)}\\\\\\vdots\\\\-y_9^{(0)}/\\hat{y}_9^{(0)}&\\cdots&-y_9^{(b-1)}/\\hat{y}_9^{(b-1)}\\end{bmatrix}\\end{align*}$$\n","\n","\n","---"],"metadata":{"id":"b9YGwzbz72CZ"}},{"cell_type":"code","source":["## Define the loss function and its gradient\n","def cce(Y, Yhat):\n","  return(?)\n","\n","def cce_gradient(Y, Yhat):\n","  return(?)\n","\n","# TensorFlow in-built function for categorical crossentropy loss\n","#cce = tf.keras.losses.CategoricalCrossentropy()"],"metadata":{"id":"hdXSGW2s7zKd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Softmax activation layer class:\n","\n","**Forward**:\n","$$\\begin{align*}\\begin{bmatrix}\\mathbf{a}^{(0)}&\\ldots&\\mathbf{a}^{(b-1)}\\end{bmatrix} &= \\begin{bmatrix}\\text{softmax}\\left(\\mathbf{z}^{(0)}\\right)&\\ldots&\\text{softmax}\\left(\\mathbf{z}^{(b-1)}\\right)\\end{bmatrix}\\\\\\Rightarrow\\mathbf{A} &= \\text{softmax}(\\mathbf{Z}).\\end{align*}$$\n","\n","**Backward**:\n","$$\\begin{align*}\\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}(L_0)&\\ldots&\\nabla_{\\mathbf{z}^{(b-1)}}(L_{b-1})\\end{bmatrix} &= \\begin{bmatrix}\\nabla_{\\mathbf{z}^{(0)}}\\left({\\mathbf{a}}^{(0)}\\right)\\times\\nabla_{\\mathbf{a}^{(0)}}(L_0)&\\cdots&\\nabla_{\\mathbf{z}^{(b-1)}}\\left({\\mathbf{a}}^{(b-1)}\\right)\\times\\nabla_{\\mathbf{a}^{(b-1)}}(L_{b-1})\\end{bmatrix}\\end{align*}$$\n","\n","![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21103299&authkey=%21AIPPR63BJ3UybA8&width=928&height=99)\n","\n","\n","---"],"metadata":{"id":"smgXLg9p65HV"}},{"cell_type":"code","source":["## Softmax activation layer class\n","class Softmax(Layer):\n","  def forward(self, input):\n","    self.input = input\n","    self.output = tf.nn.softmax(self.input, axis = 0).numpy()\n","\n","  def backward(self, output_gradient, learning_rate = None):\n","    ## Following is the inefficient way of calculating the backward gradient\n","    softmax_gradient = np.empty((?, ?), dtype = np.float64)\n","    for b in range(softmax_gradient.shape[1]):\n","      softmax_gradient[:, b] = np.dot(?, ?)\n","    return(softmax_gradient)\n","    ## Following is the efficient way of calculating the backward gradient\n","    #T = np.transpose(np.identity(self.output.shape[0]) - np.atleast_2d(self.output).T[:, np.newaxis, :], (2, 1, 0)) * np.atleast_2d(self.output)\n","    #return(np.einsum('jik, ik -> jk', T, output_gradient))"],"metadata":{"id":"4x1Xn3AbJlNy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Dense layer class:\n","\n","**Forward**:\n","$$$$\\begin{align*}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix} &= \\mathbf{W}\\begin{bmatrix}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{z}^{(b-1)}\\ldots\\end{bmatrix}\\\\&=\\begin{bmatrix}\\mathbf{W}\\mathbf{z}^{(0)}&\\ldots&\\mathbf{W}\\mathbf{z}^{(b-1)}\\end{bmatrix}\\\\\\Rightarrow \\mathbf{Z} &= \\mathbf{WX}.\\end{align*}$$$$\n","\n","**Backward**:\n","$$\\begin{align*}\\nabla_\\mathbf{W}(L)&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{W}}(\\mathbf{z}^{(0)})\\times\\nabla_{\\mathbf{z^{(0)}}}(L) +\\cdots+ \\nabla_{\\mathbf{W}}(\\mathbf{z}^{(b-1)})\\times\\nabla_{\\mathbf{z^{(b-1)}}}(L)\\right]\\\\&=\\frac{1}{b}\\left[\\nabla_{\\mathbf{z^{(0)}}}(L_0){\\mathbf{x}^{(0)}}^\\mathrm{T}+\\cdots+\\nabla_{\\mathbf{z^{(b-1)}}}(L_{b-1}) {\\mathbf{x}^{(b-1)}}^\\mathrm{T}\\right].\\end{align*}$$\n","\n","---"],"metadata":{"id":"XkPFfd1U68dj"}},{"cell_type":"code","source":["## Dense layer class\n","class Dense(Layer):\n","    def __init__(self, input_size, output_size):\n","        self.weights = np.empty((output_size, input_size+1))  # bias trick\n","        self.weights[:, :-1] = 0.01*np.random.randn(output_size, input_size)\n","        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n","\n","    def forward(self, input):\n","        self.input = ? # bias trick\n","        self.output = np.dot(?, ?)\n","\n","    def backward(self, output_gradient, learning_rate):\n","        ## Following is the inefficient way of calculating the backward gradient\n","        dense_gradient = np.zeros((?, ?), dtype = np.float64)\n","        for b in range(output_gradient.shape[1]):\n","          dense_gradient += np.dot(?, ?)\n","        dense_gradient = ?\n","        ## Following is the efficient way of calculating the backward gradient\n","        #dense_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n","        self.weights = self.weights + learning_rate * (-dense_gradient)"],"metadata":{"id":"8ctXhZYCTmHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Function to generate sample indices for batch processing according to batch size\n","\n","---"],"metadata":{"id":"2W1howeOJegI"}},{"cell_type":"code","source":["## Function to generate sample indices for batch processing according to batch size\n","def generate_batch_indices(num_samples, batch_size):\n","  # Reorder sample indices\n","  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n","  # Generate batch indices for batch processing\n","  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n","  return(batch_indices)"],"metadata":{"id":"MHyjEf22IRpc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Example generation of batch indices\n","\n","---"],"metadata":{"id":"sKFmCaFsJhkR"}},{"cell_type":"code","source":["## Example generation of batch indices\n","num_samples =\n","batch_size =\n","batch_indices = generate_batch_indices(?, ?)\n","print(batch_indices)"],"metadata":{"id":"k9QwikN0IYSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Train the 0-layer neural network using batch training with batch size = 16\n","\n","---"],"metadata":{"id":"fI_Gms9fJqbs"}},{"cell_type":"code","source":["## Train the 0-layer neural network using batch training with batch size = 16\n","learning_rate = ? # learning rate\n","batch_size = ? # batch size\n","nepochs = ? # number of epochs\n","loss_epoch = np.empty(?, dtype = np.float32) # create empty array to store losses over each epoch\n","\n","# Neural network architecture\n","dlayer = Dense(?, ?) # define dense layer\n","softmax = Softmax() # define softmax activation layer\n","\n","# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n","# and update weights.\n","\n","epoch = 0\n","while epoch < nepochs:\n","  batch_indices = generate_batch_indices(num_samples, batch_size)\n","  loss = 0\n","  for b in range(len(batch_indices)):\n","    dlayer.forward(?) # forward prop\n","    softmax.forward(?) # Softmax activate\n","    loss += cce(?, ?) # calculate loss\n","    # Backward prop starts here\n","    grad = cce_gradient(?, ?)\n","    grad = softmax.backward(?)\n","    grad = dlayer.backward(?, ?)\n","  loss_epoch[epoch] = loss/len(batch_indices)\n","  print('Epoch %d: loss = %f'%(epoch+1, loss_epoch[epoch]))\n","  epoch = epoch + 1\n"],"metadata":{"id":"LGIzrN-rPuI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Plot training loss as a function of epoch:\n","plt.plot(loss_epoch)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss value')\n","plt.show()"],"metadata":{"id":"Iv3k23SlCqGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Accuracy on test set\n"],"metadata":{"id":"d7AEbmpcKcPY"},"execution_count":null,"outputs":[]}]}